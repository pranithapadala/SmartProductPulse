{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a02dac-4fb5-49da-b699-474334edf555",
   "metadata": {},
   "source": [
    "### Importing Libraries and loading file from project path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84b86cd5-005b-41af-bcbd-21b6d174ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in ./venv_new/lib/python3.11/site-packages (from vaderSentiment) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv_new/lib/python3.11/site-packages (from requests->vaderSentiment) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv_new/lib/python3.11/site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv_new/lib/python3.11/site-packages (from requests->vaderSentiment) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv_new/lib/python3.11/site-packages (from requests->vaderSentiment) (2025.6.15)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83276a51-4e93-4c47-aa83-4ec467d6a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "# Import the tools we need\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# --- Setup file paths ---\n",
    "project_root = \"/Users/pranithapadala/Documents/SmartProductPulse\"\n",
    "RAW_DATA_DIR = os.path.join(project_root, \"data/raw\")\n",
    "PROCESSED_DATA_DIR = os.path.join(project_root, \"data/processed\")\n",
    "\n",
    "# Input file for this notebook: The raw Reddit posts\n",
    "REDDIT_FILE = os.path.join(RAW_DATA_DIR, \"raw_reddit_posts.csv\")\n",
    "\n",
    "# Output file for this notebook: Cleaned Reddit data with sentiment scores\n",
    "PROCESSED_REDDIT_FILE = os.path.join(PROCESSED_DATA_DIR, \"processed_reddit_posts.csv\")\n",
    "\n",
    "# Make sure the output folder exists\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a379d-a4d7-4935-b4d4-be58473fc702",
   "metadata": {},
   "source": [
    "### Load the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ece34af-a9ff-4460-9ce9-44f4deed46f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 115 raw Reddit posts.\n"
     ]
    }
   ],
   "source": [
    "# Load the raw Reddit posts file\n",
    "df_reddit = pd.read_csv(REDDIT_FILE)\n",
    "print(f\"Loaded {len(df_reddit)} raw Reddit posts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41b4b9-68e2-48c0-ab95-7767d1dad675",
   "metadata": {},
   "source": [
    "### Prepare Text for VADER and perform sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dfba7da-6e5a-4f65-975e-445d28b868bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Reddit Data Processing ---\n",
      "Sentiment analysis and text cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Reddit Data Processing ---\")\n",
    "\n",
    "# 1. Create a column for VADER that keeps capital letters and punctuation\n",
    "#    (We only remove URLs as they are just noise)\n",
    "df_reddit['sentiment_text'] = df_reddit['text'].astype(str).str.replace(r'https?://\\S+|www\\.\\S+', '', regex=True).str.strip()\n",
    "\n",
    "# 2. Initialize VADER and calculate the sentiment score for each post\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df_reddit['sentiment_score'] = df_reddit['sentiment_text'].apply(lambda text: analyzer.polarity_scores(text)['compound'])\n",
    "\n",
    "# 3. Create a simple text label ('Positive', 'Negative', 'Neutral')\n",
    "def get_sentiment_label(score):\n",
    "    if score >= 0.05: return 'Positive'\n",
    "    elif score <= -0.05: return 'Negative'\n",
    "    else: return 'Neutral'\n",
    "df_reddit['sentiment_label'] = df_reddit['sentiment_score'].apply(get_sentiment_label)\n",
    "\n",
    "# 4. Create a fully clean, lowercase 'cleaned_text' column for duplicate checking\n",
    "df_reddit['cleaned_text'] = df_reddit['text'].astype(str).str.lower().str.replace(r'[^a-z0-9\\s]', '', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "print(\"Sentiment analysis and text cleaning complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552a232-c53d-4231-a062-f20f28b1e2aa",
   "metadata": {},
   "source": [
    "### Save final processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d438153-3c29-4d6a-964d-942ddb723e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicate posts.\n",
      "\n",
      "--- SUCCESS! ---\n",
      "Processed Reddit data has been saved to:\n",
      "/Users/pranithapadala/Documents/SmartProductPulse/data/processed/processed_reddit_posts.csv\n",
      "This notebook's job is done. The next notebook will handle the dashboard.\n"
     ]
    }
   ],
   "source": [
    "# Remove any duplicate posts based on the fully cleaned text\n",
    "initial_rows = len(df_reddit)\n",
    "df_reddit.drop_duplicates(subset=['cleaned_text'], keep='first', inplace=True)\n",
    "final_rows = len(df_reddit)\n",
    "print(f\"Removed {initial_rows - final_rows} duplicate posts.\")\n",
    "\n",
    "# Select only the columns we need for the next stage of the project\n",
    "output_columns = ['product_id', 'sentiment_score', 'sentiment_label']\n",
    "df_processed_reddit = df_reddit[output_columns]\n",
    "\n",
    "# --- Save the clean Reddit data to a new file ---\n",
    "df_processed_reddit.to_csv(PROCESSED_REDDIT_FILE, index=False)\n",
    "\n",
    "print(f\"\\n--- SUCCESS! ---\")\n",
    "print(f\"Processed Reddit data has been saved to:\\n{PROCESSED_REDDIT_FILE}\")\n",
    "print(\"This notebook's job is done. The next notebook will handle the dashboard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489f694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6e799-efd4-4ad4-b834-21e95239496d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
